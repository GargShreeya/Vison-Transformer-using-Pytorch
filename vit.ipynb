{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Normalize\n",
    "from tqdm.notebook import tqdm \n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "patch_size=16\n",
    "channels=3\n",
    "latent_size=256\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_classes=10\n",
    "batch_size=64\n",
    "input_size=224\n",
    "n_heads=4\n",
    "base_lr=0.005\n",
    "epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Embeddings(nn.Module):\n",
    "    def __init__(self, patch_size, input_size=input_size,channels=channels, latent_size=latent_size):\n",
    "        super(Image_Embeddings, self).__init__()\n",
    "        self.p_size=patch_size\n",
    "        self.ch=channels\n",
    "        self.n_patches=(input_size**2)//patch_size\n",
    "        self.in_size=(self.p_size**2)*self.ch # flattened patches\n",
    "        self.l_size=latent_size\n",
    "        self.device=device\n",
    "        #size of the embeddings= (batch_size, n_size, patch_size**2 *n_channels)\n",
    "        self.linear=nn.Linear(self.in_size, self.l_size)\n",
    "        # After Linear Layer, in_size=(batch_size, n, latent_size)       \n",
    "        \n",
    "    def forward(self, img_data):\n",
    "        # Divide the image into patches\n",
    "        b, c, w, h=img_data.shape\n",
    "        tokens=torch.empty(b, self.n_patches, self.in_size)\n",
    "        for B in range(b):\n",
    "            n=0\n",
    "            for i in range(0, w-self.p_size, self.p_size):\n",
    "                for j in range(0, h-self.p_size, self.p_size):\n",
    "                    patch=img_data[B, :, i:i+self.p_size, j:j+self.p_size]\n",
    "                    patch=torch.flatten(patch)\n",
    "                    tokens[B,n]=patch\n",
    "                    n+=1\n",
    "                    \n",
    "        # Linear projection of the tokens\n",
    "        embeddings=self.linear(tokens)\n",
    "        # print(embeddings.shape)\n",
    "        \n",
    "        # Add the positional encodings\n",
    "        #d_model=self.l_size\n",
    "        positions=torch.arange(self.n_patches).unsqueeze(1)\n",
    "        angs = 10000**(torch.arange(self.l_size)/self.l_size).float()\n",
    "        pos_enc=torch.zeros(b, self.n_patches, self.l_size)\n",
    "        pos_enc[:,0::2]=torch.sin(positions[0::2]/angs)\n",
    "        pos_enc[:, 1::2]=torch.cos(positions[1::2]/angs)\n",
    "        embeddings+=pos_enc\n",
    "        # print(embeddings.shape)\n",
    "        \n",
    "        # Append the class token\n",
    "        # Random class token initialized\n",
    "        self.class_token=nn.Parameter(torch.randn(b, 1, self.l_size)*0.1)\n",
    "        # After appending the class token in_size= (batch_size, n+1, latent_size)\n",
    "        embeddings=torch.cat((self.class_token, embeddings ), dim=1)\n",
    "        # print(embeddings.shape)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,patch_size, n_heads, input_size=input_size,channels=channels, latent_size=latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.l_size=latent_size\n",
    "        self.ch=channels\n",
    "        self.heads=n_heads\n",
    "        self.device=device\n",
    "        \n",
    "        # Image patch embeddings\n",
    "        self.embedding=Image_Embeddings(patch_size, latent_size)\n",
    "\n",
    "        #Layer Norm\n",
    "        self.norm=nn.LayerNorm(self.l_size)\n",
    "        \n",
    "        # Self Attention Layer\n",
    "        self.attention=nn.MultiheadAttention(self.l_size, self.heads)\n",
    "        \n",
    "        #Encoder Linear Layer\n",
    "        self.mlp=nn.Sequential(\n",
    "            nn.Linear(self.l_size, self.l_size*4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.l_size*4, self.l_size))\n",
    "        \n",
    "    def forward(self, img_data):\n",
    "        embeddings=self.embedding.forward(img_data)\n",
    "        n1=self.norm(embeddings)\n",
    "        \n",
    "        n1=n1.permute(1, 0,2)\n",
    "        attn_out, att_weight=self.attention(n1,n1,n1)\n",
    "        attn_out=attn_out.permute(1,0, 2)\n",
    "        \n",
    "        a1=attn_out+embeddings\n",
    "        n2=self.norm(a1)\n",
    "        linear=self.mlp(n2)\n",
    "        a2=linear+a1\n",
    "        # return the final output and the attention weights\n",
    "        return a2, att_weight\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, n_classes,patch_size, n_heads,  channels=channels, latent_size=latent_size):\n",
    "        super(ViT, self).__init__()\n",
    "        self.l_size=latent_size\n",
    "        self.classes=n_classes\n",
    "        self.heads=n_heads\n",
    "        self.device=device\n",
    "        \n",
    "        self.encoder=Encoder(patch_size,  self.heads)\n",
    "\n",
    "        self.mlp=nn.Sequential(\n",
    "            nn.Linear(self.l_size, self.classes), \n",
    "            nn.Softmax(dim=-1))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        out, wgt=self.encoder.forward(input)\n",
    "        # Input only the class token to the linear layer\n",
    "        mlp_out=self.mlp(out[:,0])\n",
    "        return mlp_out, wgt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datastet Uplaod (CIFAR10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "manual_transforms =Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])  \n",
    "cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=manual_transforms,target_transform=transforms.Compose([\n",
    "        lambda x: torch.tensor(x)]))\n",
    "cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=manual_transforms, target_transform=transforms.Compose([\n",
    "        lambda x: torch.tensor(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(cifar_trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader=DataLoader(cifar_testset,  batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home1/shreeyagarg/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/data/home1/shreeyagarg/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(trainloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0177,  0.1881,  0.0836,  ..., -0.2007,  0.0250,  0.0124],\n",
       "         [-0.1099,  0.4627, -0.5751,  ...,  1.4927, -0.2044, -0.2491],\n",
       "         [ 0.4519,  1.0325,  0.0133,  ...,  2.4912,  0.7881,  0.7443],\n",
       "         ...,\n",
       "         [-0.6415,  0.9900,  0.9935,  ...,  0.9375,  0.9386,  0.9160],\n",
       "         [-0.9342,  0.8537,  0.8464,  ...,  0.3399,  0.3252,  0.2874],\n",
       "         [ 0.9843, -0.4255, -0.3688,  ...,  0.9375,  0.9386,  0.9159]],\n",
       "\n",
       "        [[ 0.0363, -0.0824, -0.1022,  ..., -0.0647, -0.0539, -0.0138],\n",
       "         [ 0.1449, -0.2179,  0.2876,  ..., -0.6655,  0.1462,  0.1101],\n",
       "         [ 0.6789,  0.3621,  0.8747,  ...,  0.3724,  1.1429,  1.1091],\n",
       "         ...,\n",
       "         [-0.6415,  0.9900,  0.9935,  ...,  0.9375,  0.9386,  0.9160],\n",
       "         [-0.9342,  0.8537,  0.8464,  ...,  0.3399,  0.3252,  0.2874],\n",
       "         [ 0.9843, -0.4255, -0.3688,  ...,  0.9375,  0.9386,  0.9159]],\n",
       "\n",
       "        [[-0.0244, -0.1697, -0.1165,  ...,  0.0233, -0.1119,  0.0629],\n",
       "         [ 0.0709,  0.0073, -0.0892,  ...,  0.0717,  0.2446,  0.1059],\n",
       "         [ 0.6733,  0.9340,  0.2863,  ...,  1.5064,  1.0105,  0.8951],\n",
       "         ...,\n",
       "         [-0.6415,  0.9900,  0.9935,  ...,  0.9375,  0.9386,  0.9160],\n",
       "         [-0.9342,  0.8537,  0.8464,  ...,  0.3399,  0.3252,  0.2874],\n",
       "         [ 0.9843, -0.4255, -0.3688,  ...,  0.9375,  0.9386,  0.9159]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0589, -0.0760,  0.1639,  ...,  0.0634, -0.0553,  0.1135],\n",
       "         [ 0.1182, -0.0165, -0.1007,  ..., -0.0441, -0.0105, -0.0262],\n",
       "         [ 0.5320,  0.6681,  0.5256,  ...,  1.2195,  0.8789,  0.9274],\n",
       "         ...,\n",
       "         [-0.6415,  0.9900,  0.9935,  ...,  0.9375,  0.9386,  0.9160],\n",
       "         [-0.9342,  0.8537,  0.8464,  ...,  0.3399,  0.3252,  0.2874],\n",
       "         [ 0.9843, -0.4255, -0.3688,  ...,  0.9375,  0.9386,  0.9159]],\n",
       "\n",
       "        [[-0.0197,  0.1310,  0.0028,  ...,  0.0594,  0.1873,  0.0647],\n",
       "         [ 0.1780, -0.2848,  0.2974,  ..., -0.7843,  0.1209,  0.1160],\n",
       "         [ 0.6476,  0.3896,  0.8810,  ...,  0.3444,  1.1112,  1.1001],\n",
       "         ...,\n",
       "         [-0.6415,  0.9900,  0.9935,  ...,  0.9375,  0.9386,  0.9160],\n",
       "         [-0.9342,  0.8537,  0.8464,  ...,  0.3399,  0.3252,  0.2874],\n",
       "         [ 0.9843, -0.4255, -0.3688,  ...,  0.9375,  0.9386,  0.9159]],\n",
       "\n",
       "        [[-0.0986,  0.1031, -0.0432,  ..., -0.2463,  0.0177,  0.0992],\n",
       "         [-0.0477,  0.0228,  0.1889,  ..., -0.1321,  0.1427,  0.0174],\n",
       "         [ 0.6993,  0.4085,  0.6526,  ...,  0.6931,  1.0210,  1.0108],\n",
       "         ...,\n",
       "         [-0.6415,  0.9900,  0.9935,  ...,  0.9375,  0.9386,  0.9160],\n",
       "         [-0.9342,  0.8537,  0.8464,  ...,  0.3399,  0.3252,  0.2874],\n",
       "         [ 0.9843, -0.4255, -0.3688,  ...,  0.9375,  0.9386,  0.9159]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em=Image_Embeddings(patch_size)\n",
    "em.forward(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, dataloader=trainloader, epochs=epochs,base_lr=base_lr, device=device, criterion = nn.CrossEntropyLoss()):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=base_lr)\n",
    "    train_losses = []\n",
    "    acc=[]\n",
    "    for epoch in tqdm(range(epochs), total=epochs):\n",
    "        train_loss = 0.0\n",
    "        total_samples=0\n",
    "        total_correct=0\n",
    "        num_batches=0\n",
    "        print(\"Epoch {}:\".format(epoch))\n",
    "\n",
    "        for (batch_img, batch_labels) in tqdm(dataloader):\n",
    "            # batch_img, batch_labels=batch_img.to(device), batch_labels.to(device)\n",
    "            outputs, _ = model.forward(batch_img)  \n",
    "            \n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            train_loss += loss.detach().cpu().item() / len(dataloader)\n",
    "            # print(outputs.shape)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += batch_labels.size(0)\n",
    "            total_correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "            num_batches+=1\n",
    "            # print('Batch {} :  loss={}'.format(batch_idx,  running_loss))   \n",
    "        \n",
    "        \n",
    "        train_losses.append(train_loss)             \n",
    "\n",
    "        accuracy = total_correct / total_samples\n",
    "        acc.append(accuracy)\n",
    "        print(f\"Accuracy on train set: {accuracy*100:.2f}%\")\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} loss: {train_loss:.2f}\")\n",
    "    return train_losses, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitModel1=ViT(n_classes,patch_size, n_heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d875f18146740eeb2c01b3ade106ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3912ad4ec9ca491987e5fa57971a871d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home1/shreeyagarg/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/data/home1/shreeyagarg/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loss1, acc1=train(vitModel1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
